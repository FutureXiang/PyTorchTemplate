{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitpytorchcondada6ebc6d20d843ec8febc7b83b049018",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def genDataFunction(x, lam=0.5, const=10):\n",
    "    return lam * np.exp(-lam * x) * const + np.random.random()/100\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train_x = list((np.random.random(10000) * 10).astype(int))\n",
    "        self.train_y = [genDataFunction(x) for x in self.train_x]\n",
    "        self.test_x = list((np.random.random(2000) * 10).astype(int))\n",
    "        self.test_y = [genDataFunction(x) for x in self.test_x]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.train_x[index]\n",
    "        y = self.train_y[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.train_x) == len(self.train_y)\n",
    "        return len(self.train_x)\n",
    "\n",
    "\n",
    "class DataProvider:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = MyDataset()\n",
    "        self.dataiter = None\n",
    "\n",
    "        self.train_len = self.dataset.__len__()\n",
    "        self.train_batch_num = self.train_len // self.batch_size\n",
    "\n",
    "    def build(self):\n",
    "        self.dataloader = DataLoader(self.dataset,\n",
    "                                     batch_size=self.batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "        self.dataiter = iter(self.dataloader)\n",
    "\n",
    "    def next(self):\n",
    "        if self.dataiter is None:\n",
    "            self.build()\n",
    "        try:\n",
    "            batch = next(self.dataiter)\n",
    "            batch = (batch[0].float(), batch[1].float())\n",
    "            return batch\n",
    "        except StopIteration:\n",
    "            self.build()\n",
    "            batch = next(self.dataiter)\n",
    "            batch = (batch[0].float(), batch[1].float())\n",
    "            return batch\n",
    "\n",
    "\n",
    "provider = DataProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([6., 2., 1., 1., 6.]) tensor([0.2533, 1.8440, 3.0354, 3.0380, 0.2583])\n"
     ]
    }
   ],
   "source": [
    "xs, ys = provider.next()\n",
    "print(xs[:5], ys[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitModel(nn.Module):\n",
    "    '''\n",
    "        define model here\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(FitModel, self).__init__()\n",
    "        self.lam = nn.Parameter(torch.FloatTensor([1.]), requires_grad=True)\n",
    "        self.const = nn.Parameter(torch.FloatTensor([1.]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lam * torch.exp(-self.lam * x) * self.const\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if 'lr' in param_group.keys():\n",
    "            return param_group['lr']\n",
    "\n",
    "\n",
    "class LogMSELoss(nn.Module):\n",
    "    '''\n",
    "        define custom loss function here\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LogMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        return torch.mean(torch.pow(torch.log(preds) - torch.log(targets), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(criterionFunc = \"MSE\", epochMax = 50):\n",
    "    if criterionFunc == \"MSE\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif criterionFunc == \"LogMSE\":\n",
    "        criterion = LogMSELoss()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model = FitModel()  # create a new model (retrain parameters)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3, verbose=True)\n",
    "\n",
    "    WATCH_LOSS_PER_BATCH = 5\n",
    "    for epoch in range(epochMax):\n",
    "        sum_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_no in range(1, provider.train_batch_num + 1):\n",
    "            xs, ys = provider.next()\n",
    "            pred = model.forward(xs)\n",
    "            loss = criterion(pred, ys)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if batch_no % WATCH_LOSS_PER_BATCH == 0 or batch_no == provider.train_batch_num:\n",
    "                print(\"lam = {:5f}\".format(model.lam.item()))\n",
    "                print(\"con = {:5f}\".format(model.const.item()))\n",
    "\n",
    "                print(\"[epoch:{}, batch:{}/{}] loss: {:.5f} lr: {:.5f}\".format(\n",
    "                    epoch, batch_no, provider.train_batch_num, sum_loss / WATCH_LOSS_PER_BATCH, get_lr(optimizer)))\n",
    "                sum_loss = 0.0\n",
    "        scheduler.step(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 0.00151 lr: 0.01000\n",
      "lam = 0.515677\n",
      "con = 9.695725\n",
      "[epoch:34, batch:40/78] loss: 0.00154 lr: 0.01000\n",
      "lam = 0.515414\n",
      "con = 9.699303\n",
      "[epoch:34, batch:45/78] loss: 0.00147 lr: 0.01000\n",
      "lam = 0.515174\n",
      "con = 9.702762\n",
      "[epoch:34, batch:50/78] loss: 0.00138 lr: 0.01000\n",
      "lam = 0.514885\n",
      "con = 9.706242\n",
      "[epoch:34, batch:55/78] loss: 0.00144 lr: 0.01000\n",
      "lam = 0.514142\n",
      "con = 9.709711\n",
      "[epoch:34, batch:60/78] loss: 0.00139 lr: 0.01000\n",
      "lam = 0.515126\n",
      "con = 9.713155\n",
      "[epoch:34, batch:65/78] loss: 0.00133 lr: 0.01000\n",
      "lam = 0.513491\n",
      "con = 9.716452\n",
      "[epoch:34, batch:70/78] loss: 0.00126 lr: 0.01000\n",
      "lam = 0.514540\n",
      "con = 9.719794\n",
      "[epoch:34, batch:75/78] loss: 0.00127 lr: 0.01000\n",
      "lam = 0.514088\n",
      "con = 9.721755\n",
      "[epoch:34, batch:78/78] loss: 0.00075 lr: 0.01000\n",
      "lam = 0.513274\n",
      "con = 9.724997\n",
      "[epoch:35, batch:5/78] loss: 0.00123 lr: 0.01000\n",
      "lam = 0.514230\n",
      "con = 9.728339\n",
      "[epoch:35, batch:10/78] loss: 0.00130 lr: 0.01000\n",
      "lam = 0.513498\n",
      "con = 9.731589\n",
      "[epoch:35, batch:15/78] loss: 0.00121 lr: 0.01000\n",
      "lam = 0.513270\n",
      "con = 9.734854\n",
      "[epoch:35, batch:20/78] loss: 0.00116 lr: 0.01000\n",
      "lam = 0.513732\n",
      "con = 9.738118\n",
      "[epoch:35, batch:25/78] loss: 0.00116 lr: 0.01000\n",
      "lam = 0.513037\n",
      "con = 9.741286\n",
      "[epoch:35, batch:30/78] loss: 0.00110 lr: 0.01000\n",
      "lam = 0.513502\n",
      "con = 9.744446\n",
      "[epoch:35, batch:35/78] loss: 0.00112 lr: 0.01000\n",
      "lam = 0.512622\n",
      "con = 9.747614\n",
      "[epoch:35, batch:40/78] loss: 0.00114 lr: 0.01000\n",
      "lam = 0.512596\n",
      "con = 9.750769\n",
      "[epoch:35, batch:45/78] loss: 0.00106 lr: 0.01000\n",
      "lam = 0.512697\n",
      "con = 9.753915\n",
      "[epoch:35, batch:50/78] loss: 0.00104 lr: 0.01000\n",
      "lam = 0.512355\n",
      "con = 9.756977\n",
      "[epoch:35, batch:55/78] loss: 0.00097 lr: 0.01000\n",
      "lam = 0.512379\n",
      "con = 9.759957\n",
      "[epoch:35, batch:60/78] loss: 0.00096 lr: 0.01000\n",
      "lam = 0.511728\n",
      "con = 9.762922\n",
      "[epoch:35, batch:65/78] loss: 0.00096 lr: 0.01000\n",
      "lam = 0.511173\n",
      "con = 9.765880\n",
      "[epoch:35, batch:70/78] loss: 0.00100 lr: 0.01000\n",
      "lam = 0.512354\n",
      "con = 9.768896\n",
      "[epoch:35, batch:75/78] loss: 0.00094 lr: 0.01000\n",
      "lam = 0.511166\n",
      "con = 9.770623\n",
      "[epoch:35, batch:78/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.511380\n",
      "con = 9.773579\n",
      "[epoch:36, batch:5/78] loss: 0.00094 lr: 0.01000\n",
      "lam = 0.511419\n",
      "con = 9.776506\n",
      "[epoch:36, batch:10/78] loss: 0.00086 lr: 0.01000\n",
      "lam = 0.510754\n",
      "con = 9.779367\n",
      "[epoch:36, batch:15/78] loss: 0.00084 lr: 0.01000\n",
      "lam = 0.511402\n",
      "con = 9.782178\n",
      "[epoch:36, batch:20/78] loss: 0.00078 lr: 0.01000\n",
      "lam = 0.511322\n",
      "con = 9.784920\n",
      "[epoch:36, batch:25/78] loss: 0.00082 lr: 0.01000\n",
      "lam = 0.509865\n",
      "con = 9.787648\n",
      "[epoch:36, batch:30/78] loss: 0.00083 lr: 0.01000\n",
      "lam = 0.510867\n",
      "con = 9.790454\n",
      "[epoch:36, batch:35/78] loss: 0.00079 lr: 0.01000\n",
      "lam = 0.510343\n",
      "con = 9.793205\n",
      "[epoch:36, batch:40/78] loss: 0.00078 lr: 0.01000\n",
      "lam = 0.511028\n",
      "con = 9.795951\n",
      "[epoch:36, batch:45/78] loss: 0.00075 lr: 0.01000\n",
      "lam = 0.509674\n",
      "con = 9.798658\n",
      "[epoch:36, batch:50/78] loss: 0.00078 lr: 0.01000\n",
      "lam = 0.510561\n",
      "con = 9.801423\n",
      "[epoch:36, batch:55/78] loss: 0.00072 lr: 0.01000\n",
      "lam = 0.509345\n",
      "con = 9.804061\n",
      "[epoch:36, batch:60/78] loss: 0.00072 lr: 0.01000\n",
      "lam = 0.510781\n",
      "con = 9.806754\n",
      "[epoch:36, batch:65/78] loss: 0.00070 lr: 0.01000\n",
      "lam = 0.508754\n",
      "con = 9.809335\n",
      "[epoch:36, batch:70/78] loss: 0.00068 lr: 0.01000\n",
      "lam = 0.509833\n",
      "con = 9.811959\n",
      "[epoch:36, batch:75/78] loss: 0.00067 lr: 0.01000\n",
      "lam = 0.510247\n",
      "con = 9.813532\n",
      "[epoch:36, batch:78/78] loss: 0.00040 lr: 0.01000\n",
      "lam = 0.508558\n",
      "con = 9.816077\n",
      "[epoch:37, batch:5/78] loss: 0.00065 lr: 0.01000\n",
      "lam = 0.509668\n",
      "con = 9.818653\n",
      "[epoch:37, batch:10/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.508732\n",
      "con = 9.821191\n",
      "[epoch:37, batch:15/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.509356\n",
      "con = 9.823776\n",
      "[epoch:37, batch:20/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.508428\n",
      "con = 9.826283\n",
      "[epoch:37, batch:25/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.508883\n",
      "con = 9.828762\n",
      "[epoch:37, batch:30/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.508568\n",
      "con = 9.831077\n",
      "[epoch:37, batch:35/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.508432\n",
      "con = 9.833379\n",
      "[epoch:37, batch:40/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.508301\n",
      "con = 9.835723\n",
      "[epoch:37, batch:45/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.508400\n",
      "con = 9.838070\n",
      "[epoch:37, batch:50/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.508349\n",
      "con = 9.840401\n",
      "[epoch:37, batch:55/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.508396\n",
      "con = 9.842726\n",
      "[epoch:37, batch:60/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.507658\n",
      "con = 9.844991\n",
      "[epoch:37, batch:65/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.507802\n",
      "con = 9.847276\n",
      "[epoch:37, batch:70/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.507732\n",
      "con = 9.849537\n",
      "[epoch:37, batch:75/78] loss: 0.00045 lr: 0.01000\n",
      "lam = 0.506846\n",
      "con = 9.850825\n",
      "[epoch:37, batch:78/78] loss: 0.00026 lr: 0.01000\n",
      "lam = 0.507429\n",
      "con = 9.853040\n",
      "[epoch:38, batch:5/78] loss: 0.00045 lr: 0.01000\n",
      "lam = 0.507469\n",
      "con = 9.855239\n",
      "[epoch:38, batch:10/78] loss: 0.00044 lr: 0.01000\n",
      "lam = 0.507266\n",
      "con = 9.857397\n",
      "[epoch:38, batch:15/78] loss: 0.00042 lr: 0.01000\n",
      "lam = 0.507324\n",
      "con = 9.859529\n",
      "[epoch:38, batch:20/78] loss: 0.00042 lr: 0.01000\n",
      "lam = 0.507160\n",
      "con = 9.861652\n",
      "[epoch:38, batch:25/78] loss: 0.00041 lr: 0.01000\n",
      "lam = 0.507135\n",
      "con = 9.863769\n",
      "[epoch:38, batch:30/78] loss: 0.00040 lr: 0.01000\n",
      "lam = 0.506672\n",
      "con = 9.865900\n",
      "[epoch:38, batch:35/78] loss: 0.00041 lr: 0.01000\n",
      "lam = 0.506940\n",
      "con = 9.868011\n",
      "[epoch:38, batch:40/78] loss: 0.00038 lr: 0.01000\n",
      "lam = 0.506279\n",
      "con = 9.870116\n",
      "[epoch:38, batch:45/78] loss: 0.00039 lr: 0.01000\n",
      "lam = 0.506989\n",
      "con = 9.872219\n",
      "[epoch:38, batch:50/78] loss: 0.00035 lr: 0.01000\n",
      "lam = 0.506134\n",
      "con = 9.874175\n",
      "[epoch:38, batch:55/78] loss: 0.00033 lr: 0.01000\n",
      "lam = 0.506484\n",
      "con = 9.876163\n",
      "[epoch:38, batch:60/78] loss: 0.00034 lr: 0.01000\n",
      "lam = 0.506133\n",
      "con = 9.878088\n",
      "[epoch:38, batch:65/78] loss: 0.00033 lr: 0.01000\n",
      "lam = 0.505780\n",
      "con = 9.880013\n",
      "[epoch:38, batch:70/78] loss: 0.00034 lr: 0.01000\n",
      "lam = 0.506342\n",
      "con = 9.881946\n",
      "[epoch:38, batch:75/78] loss: 0.00033 lr: 0.01000\n",
      "lam = 0.506069\n",
      "con = 9.883091\n",
      "[epoch:38, batch:78/78] loss: 0.00019 lr: 0.01000\n",
      "lam = 0.506126\n",
      "con = 9.885016\n",
      "[epoch:39, batch:5/78] loss: 0.00030 lr: 0.01000\n",
      "lam = 0.505688\n",
      "con = 9.886892\n",
      "[epoch:39, batch:10/78] loss: 0.00030 lr: 0.01000\n",
      "lam = 0.505893\n",
      "con = 9.888759\n",
      "[epoch:39, batch:15/78] loss: 0.00028 lr: 0.01000\n",
      "lam = 0.505639\n",
      "con = 9.890587\n",
      "[epoch:39, batch:20/78] loss: 0.00029 lr: 0.01000\n",
      "lam = 0.505744\n",
      "con = 9.892442\n",
      "[epoch:39, batch:25/78] loss: 0.00030 lr: 0.01000\n",
      "lam = 0.505394\n",
      "con = 9.894269\n",
      "[epoch:39, batch:30/78] loss: 0.00027 lr: 0.01000\n",
      "lam = 0.505472\n",
      "con = 9.896073\n",
      "[epoch:39, batch:35/78] loss: 0.00027 lr: 0.01000\n",
      "lam = 0.505277\n",
      "con = 9.897860\n",
      "[epoch:39, batch:40/78] loss: 0.00028 lr: 0.01000\n",
      "lam = 0.504837\n",
      "con = 9.899673\n",
      "[epoch:39, batch:45/78] loss: 0.00028 lr: 0.01000\n",
      "lam = 0.505240\n",
      "con = 9.901488\n",
      "[epoch:39, batch:50/78] loss: 0.00025 lr: 0.01000\n",
      "lam = 0.504653\n",
      "con = 9.903223\n",
      "[epoch:39, batch:55/78] loss: 0.00024 lr: 0.01000\n",
      "lam = 0.504923\n",
      "con = 9.904948\n",
      "[epoch:39, batch:60/78] loss: 0.00023 lr: 0.01000\n",
      "lam = 0.504849\n",
      "con = 9.906629\n",
      "[epoch:39, batch:65/78] loss: 0.00022 lr: 0.01000\n",
      "lam = 0.504982\n",
      "con = 9.908262\n",
      "[epoch:39, batch:70/78] loss: 0.00021 lr: 0.01000\n",
      "lam = 0.504790\n",
      "con = 9.909855\n",
      "[epoch:39, batch:75/78] loss: 0.00022 lr: 0.01000\n",
      "lam = 0.504318\n",
      "con = 9.910790\n",
      "[epoch:39, batch:78/78] loss: 0.00012 lr: 0.01000\n",
      "lam = 0.504916\n",
      "con = 9.912389\n",
      "[epoch:40, batch:5/78] loss: 0.00021 lr: 0.01000\n",
      "lam = 0.504184\n",
      "con = 9.913968\n",
      "[epoch:40, batch:10/78] loss: 0.00021 lr: 0.01000\n",
      "lam = 0.504746\n",
      "con = 9.915559\n",
      "[epoch:40, batch:15/78] loss: 0.00019 lr: 0.01000\n",
      "lam = 0.504287\n",
      "con = 9.917077\n",
      "[epoch:40, batch:20/78] loss: 0.00020 lr: 0.01000\n",
      "lam = 0.504315\n",
      "con = 9.918604\n",
      "[epoch:40, batch:25/78] loss: 0.00019 lr: 0.01000\n",
      "lam = 0.503716\n",
      "con = 9.920116\n",
      "[epoch:40, batch:30/78] loss: 0.00019 lr: 0.01000\n",
      "lam = 0.504601\n",
      "con = 9.921700\n",
      "[epoch:40, batch:35/78] loss: 0.00019 lr: 0.01000\n",
      "lam = 0.503434\n",
      "con = 9.923168\n",
      "[epoch:40, batch:40/78] loss: 0.00017 lr: 0.01000\n",
      "lam = 0.504090\n",
      "con = 9.924708\n",
      "[epoch:40, batch:45/78] loss: 0.00018 lr: 0.01000\n",
      "lam = 0.504131\n",
      "con = 9.926203\n",
      "[epoch:40, batch:50/78] loss: 0.00018 lr: 0.01000\n",
      "lam = 0.503660\n",
      "con = 9.927692\n",
      "[epoch:40, batch:55/78] loss: 0.00017 lr: 0.01000\n",
      "lam = 0.503775\n",
      "con = 9.929183\n",
      "[epoch:40, batch:60/78] loss: 0.00017 lr: 0.01000\n",
      "lam = 0.503593\n",
      "con = 9.930639\n",
      "[epoch:40, batch:65/78] loss: 0.00016 lr: 0.01000\n",
      "lam = 0.503574\n",
      "con = 9.932071\n",
      "[epoch:40, batch:70/78] loss: 0.00015 lr: 0.01000\n",
      "lam = 0.503603\n",
      "con = 9.933483\n",
      "[epoch:40, batch:75/78] loss: 0.00015 lr: 0.01000\n",
      "lam = 0.503386\n",
      "con = 9.934304\n",
      "[epoch:40, batch:78/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.503567\n",
      "con = 9.935678\n",
      "[epoch:41, batch:5/78] loss: 0.00014 lr: 0.01000\n",
      "lam = 0.503278\n",
      "con = 9.937016\n",
      "[epoch:41, batch:10/78] loss: 0.00014 lr: 0.01000\n",
      "lam = 0.503236\n",
      "con = 9.938369\n",
      "[epoch:41, batch:15/78] loss: 0.00014 lr: 0.01000\n",
      "lam = 0.503459\n",
      "con = 9.939689\n",
      "[epoch:41, batch:20/78] loss: 0.00013 lr: 0.01000\n",
      "lam = 0.502881\n",
      "con = 9.940972\n",
      "[epoch:41, batch:25/78] loss: 0.00013 lr: 0.01000\n",
      "lam = 0.503163\n",
      "con = 9.942291\n",
      "[epoch:41, batch:30/78] loss: 0.00013 lr: 0.01000\n",
      "lam = 0.502841\n",
      "con = 9.943571\n",
      "[epoch:41, batch:35/78] loss: 0.00012 lr: 0.01000\n",
      "lam = 0.503114\n",
      "con = 9.944858\n",
      "[epoch:41, batch:40/78] loss: 0.00013 lr: 0.01000\n",
      "lam = 0.502935\n",
      "con = 9.946135\n",
      "[epoch:41, batch:45/78] loss: 0.00012 lr: 0.01000\n",
      "lam = 0.502939\n",
      "con = 9.947395\n",
      "[epoch:41, batch:50/78] loss: 0.00012 lr: 0.01000\n",
      "lam = 0.502670\n",
      "con = 9.948635\n",
      "[epoch:41, batch:55/78] loss: 0.00012 lr: 0.01000\n",
      "lam = 0.502809\n",
      "con = 9.949881\n",
      "[epoch:41, batch:60/78] loss: 0.00012 lr: 0.01000\n",
      "lam = 0.502535\n",
      "con = 9.951075\n",
      "[epoch:41, batch:65/78] loss: 0.00010 lr: 0.01000\n",
      "lam = 0.502872\n",
      "con = 9.952291\n",
      "[epoch:41, batch:70/78] loss: 0.00011 lr: 0.01000\n",
      "lam = 0.502104\n",
      "con = 9.953450\n",
      "[epoch:41, batch:75/78] loss: 0.00011 lr: 0.01000\n",
      "lam = 0.502278\n",
      "con = 9.954160\n",
      "[epoch:41, batch:78/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.502665\n",
      "con = 9.955329\n",
      "[epoch:42, batch:5/78] loss: 0.00010 lr: 0.01000\n",
      "lam = 0.502602\n",
      "con = 9.956480\n",
      "[epoch:42, batch:10/78] loss: 0.00010 lr: 0.01000\n",
      "lam = 0.502211\n",
      "con = 9.957610\n",
      "[epoch:42, batch:15/78] loss: 0.00010 lr: 0.01000\n",
      "lam = 0.502445\n",
      "con = 9.958757\n",
      "[epoch:42, batch:20/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.502137\n",
      "con = 9.959865\n",
      "[epoch:42, batch:25/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.502298\n",
      "con = 9.960970\n",
      "[epoch:42, batch:30/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.502096\n",
      "con = 9.962068\n",
      "[epoch:42, batch:35/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.502036\n",
      "con = 9.963140\n",
      "[epoch:42, batch:40/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.501864\n",
      "con = 9.964210\n",
      "[epoch:42, batch:45/78] loss: 0.00009 lr: 0.01000\n",
      "lam = 0.502145\n",
      "con = 9.965273\n",
      "[epoch:42, batch:50/78] loss: 0.00008 lr: 0.01000\n",
      "lam = 0.501744\n",
      "con = 9.966300\n",
      "[epoch:42, batch:55/78] loss: 0.00008 lr: 0.01000\n",
      "lam = 0.502061\n",
      "con = 9.967326\n",
      "[epoch:42, batch:60/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501457\n",
      "con = 9.968318\n",
      "[epoch:42, batch:65/78] loss: 0.00008 lr: 0.01000\n",
      "lam = 0.502023\n",
      "con = 9.969334\n",
      "[epoch:42, batch:70/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501656\n",
      "con = 9.970284\n",
      "[epoch:42, batch:75/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501620\n",
      "con = 9.970862\n",
      "[epoch:42, batch:78/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.501810\n",
      "con = 9.971828\n",
      "[epoch:43, batch:5/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501447\n",
      "con = 9.972775\n",
      "[epoch:43, batch:10/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501625\n",
      "con = 9.973728\n",
      "[epoch:43, batch:15/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.501433\n",
      "con = 9.974666\n",
      "[epoch:43, batch:20/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501571\n",
      "con = 9.975624\n",
      "[epoch:43, batch:25/78] loss: 0.00007 lr: 0.01000\n",
      "lam = 0.501485\n",
      "con = 9.976541\n",
      "[epoch:43, batch:30/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.501443\n",
      "con = 9.977442\n",
      "[epoch:43, batch:35/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.501208\n",
      "con = 9.978320\n",
      "[epoch:43, batch:40/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.501378\n",
      "con = 9.979197\n",
      "[epoch:43, batch:45/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.501185\n",
      "con = 9.980053\n",
      "[epoch:43, batch:50/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.501220\n",
      "con = 9.980908\n",
      "[epoch:43, batch:55/78] loss: 0.00006 lr: 0.01000\n",
      "lam = 0.501087\n",
      "con = 9.981755\n",
      "[epoch:43, batch:60/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.501089\n",
      "con = 9.982589\n",
      "[epoch:43, batch:65/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.501316\n",
      "con = 9.983409\n",
      "[epoch:43, batch:70/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.500935\n",
      "con = 9.984210\n",
      "[epoch:43, batch:75/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.501211\n",
      "con = 9.984702\n",
      "[epoch:43, batch:78/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500888\n",
      "con = 9.985485\n",
      "[epoch:44, batch:5/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.501017\n",
      "con = 9.986282\n",
      "[epoch:44, batch:10/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.500970\n",
      "con = 9.987062\n",
      "[epoch:44, batch:15/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.500890\n",
      "con = 9.987828\n",
      "[epoch:44, batch:20/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.500851\n",
      "con = 9.988584\n",
      "[epoch:44, batch:25/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.500815\n",
      "con = 9.989327\n",
      "[epoch:44, batch:30/78] loss: 0.00005 lr: 0.01000\n",
      "lam = 0.500715\n",
      "con = 9.990067\n",
      "[epoch:44, batch:35/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500511\n",
      "con = 9.990793\n",
      "[epoch:44, batch:40/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500922\n",
      "con = 9.991520\n",
      "[epoch:44, batch:45/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500583\n",
      "con = 9.992225\n",
      "[epoch:44, batch:50/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500635\n",
      "con = 9.992936\n",
      "[epoch:44, batch:55/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500729\n",
      "con = 9.993647\n",
      "[epoch:44, batch:60/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500374\n",
      "con = 9.994332\n",
      "[epoch:44, batch:65/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500897\n",
      "con = 9.995036\n",
      "[epoch:44, batch:70/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500264\n",
      "con = 9.995691\n",
      "[epoch:44, batch:75/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500465\n",
      "con = 9.996097\n",
      "[epoch:44, batch:78/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.500341\n",
      "con = 9.996737\n",
      "[epoch:45, batch:5/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500567\n",
      "con = 9.997387\n",
      "[epoch:45, batch:10/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500389\n",
      "con = 9.998025\n",
      "[epoch:45, batch:15/78] loss: 0.00004 lr: 0.01000\n",
      "lam = 0.500600\n",
      "con = 9.998661\n",
      "[epoch:45, batch:20/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500064\n",
      "con = 9.999269\n",
      "[epoch:45, batch:25/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500584\n",
      "con = 9.999915\n",
      "[epoch:45, batch:30/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500073\n",
      "con = 10.000515\n",
      "[epoch:45, batch:35/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500373\n",
      "con = 10.001126\n",
      "[epoch:45, batch:40/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500167\n",
      "con = 10.001713\n",
      "[epoch:45, batch:45/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500122\n",
      "con = 10.002300\n",
      "[epoch:45, batch:50/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500302\n",
      "con = 10.002875\n",
      "[epoch:45, batch:55/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500087\n",
      "con = 10.003425\n",
      "[epoch:45, batch:60/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499881\n",
      "con = 10.003965\n",
      "[epoch:45, batch:65/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500408\n",
      "con = 10.004528\n",
      "[epoch:45, batch:70/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499735\n",
      "con = 10.005046\n",
      "[epoch:45, batch:75/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499855\n",
      "con = 10.005370\n",
      "[epoch:45, batch:78/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499977\n",
      "con = 10.005898\n",
      "[epoch:46, batch:5/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500102\n",
      "con = 10.006424\n",
      "[epoch:46, batch:10/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499840\n",
      "con = 10.006941\n",
      "[epoch:46, batch:15/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500105\n",
      "con = 10.007461\n",
      "[epoch:46, batch:20/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499698\n",
      "con = 10.007951\n",
      "[epoch:46, batch:25/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.500132\n",
      "con = 10.008478\n",
      "[epoch:46, batch:30/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499850\n",
      "con = 10.008960\n",
      "[epoch:46, batch:35/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499932\n",
      "con = 10.009449\n",
      "[epoch:46, batch:40/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499703\n",
      "con = 10.009926\n",
      "[epoch:46, batch:45/78] loss: 0.00003 lr: 0.01000\n",
      "lam = 0.499872\n",
      "con = 10.010411\n",
      "[epoch:46, batch:50/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499782\n",
      "con = 10.010880\n",
      "[epoch:46, batch:55/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499532\n",
      "con = 10.011334\n",
      "[epoch:46, batch:60/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499915\n",
      "con = 10.011806\n",
      "[epoch:46, batch:65/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499697\n",
      "con = 10.012237\n",
      "[epoch:46, batch:70/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499702\n",
      "con = 10.012653\n",
      "[epoch:46, batch:75/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499523\n",
      "con = 10.012895\n",
      "[epoch:46, batch:78/78] loss: 0.00001 lr: 0.01000\n",
      "lam = 0.499906\n",
      "con = 10.013322\n",
      "[epoch:47, batch:5/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499310\n",
      "con = 10.013717\n",
      "[epoch:47, batch:10/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499876\n",
      "con = 10.014139\n",
      "[epoch:47, batch:15/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499152\n",
      "con = 10.014513\n",
      "[epoch:47, batch:20/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499809\n",
      "con = 10.014930\n",
      "[epoch:47, batch:25/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499424\n",
      "con = 10.015314\n",
      "[epoch:47, batch:30/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499605\n",
      "con = 10.015714\n",
      "[epoch:47, batch:35/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499410\n",
      "con = 10.016085\n",
      "[epoch:47, batch:40/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499758\n",
      "con = 10.016471\n",
      "[epoch:47, batch:45/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499229\n",
      "con = 10.016830\n",
      "[epoch:47, batch:50/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499738\n",
      "con = 10.017216\n",
      "[epoch:47, batch:55/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499425\n",
      "con = 10.017570\n",
      "[epoch:47, batch:60/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499427\n",
      "con = 10.017926\n",
      "[epoch:47, batch:65/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499323\n",
      "con = 10.018276\n",
      "[epoch:47, batch:70/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499556\n",
      "con = 10.018639\n",
      "[epoch:47, batch:75/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499372\n",
      "con = 10.018841\n",
      "[epoch:47, batch:78/78] loss: 0.00001 lr: 0.01000\n",
      "lam = 0.499482\n",
      "con = 10.019181\n",
      "[epoch:48, batch:5/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499503\n",
      "con = 10.019504\n",
      "[epoch:48, batch:10/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499299\n",
      "con = 10.019805\n",
      "[epoch:48, batch:15/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499445\n",
      "con = 10.020128\n",
      "[epoch:48, batch:20/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499156\n",
      "con = 10.020435\n",
      "[epoch:48, batch:25/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499287\n",
      "con = 10.020757\n",
      "[epoch:48, batch:30/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499204\n",
      "con = 10.021064\n",
      "[epoch:48, batch:35/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499300\n",
      "con = 10.021376\n",
      "[epoch:48, batch:40/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499128\n",
      "con = 10.021667\n",
      "[epoch:48, batch:45/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499435\n",
      "con = 10.021983\n",
      "[epoch:48, batch:50/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499266\n",
      "con = 10.022279\n",
      "[epoch:48, batch:55/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499103\n",
      "con = 10.022546\n",
      "[epoch:48, batch:60/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499224\n",
      "con = 10.022820\n",
      "[epoch:48, batch:65/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499060\n",
      "con = 10.023093\n",
      "[epoch:48, batch:70/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499198\n",
      "con = 10.023380\n",
      "[epoch:48, batch:75/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499053\n",
      "con = 10.023545\n",
      "[epoch:48, batch:78/78] loss: 0.00001 lr: 0.01000\n",
      "lam = 0.499267\n",
      "con = 10.023829\n",
      "[epoch:49, batch:5/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498987\n",
      "con = 10.024087\n",
      "[epoch:49, batch:10/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499209\n",
      "con = 10.024362\n",
      "[epoch:49, batch:15/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498950\n",
      "con = 10.024617\n",
      "[epoch:49, batch:20/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499154\n",
      "con = 10.024864\n",
      "[epoch:49, batch:25/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498913\n",
      "con = 10.025096\n",
      "[epoch:49, batch:30/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499107\n",
      "con = 10.025339\n",
      "[epoch:49, batch:35/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499121\n",
      "con = 10.025576\n",
      "[epoch:49, batch:40/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498893\n",
      "con = 10.025801\n",
      "[epoch:49, batch:45/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499123\n",
      "con = 10.026038\n",
      "[epoch:49, batch:50/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499155\n",
      "con = 10.026260\n",
      "[epoch:49, batch:55/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498817\n",
      "con = 10.026476\n",
      "[epoch:49, batch:60/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499209\n",
      "con = 10.026711\n",
      "[epoch:49, batch:65/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498939\n",
      "con = 10.026919\n",
      "[epoch:49, batch:70/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.499006\n",
      "con = 10.027136\n",
      "[epoch:49, batch:75/78] loss: 0.00002 lr: 0.01000\n",
      "lam = 0.498760\n",
      "con = 10.027255\n",
      "[epoch:49, batch:78/78] loss: 0.00001 lr: 0.01000\n"
     ]
    }
   ],
   "source": [
    "fit(criterionFunc='MSE', epochMax=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[epoch:84, batch:50/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.489746\n",
      "con = 9.967126\n",
      "[epoch:84, batch:55/78] loss: 0.00069 lr: 0.01000\n",
      "lam = 0.491319\n",
      "con = 9.967559\n",
      "[epoch:84, batch:60/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.491984\n",
      "con = 9.968042\n",
      "[epoch:84, batch:65/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.489747\n",
      "con = 9.968682\n",
      "[epoch:84, batch:70/78] loss: 0.00071 lr: 0.01000\n",
      "lam = 0.490861\n",
      "con = 9.969113\n",
      "[epoch:84, batch:75/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491598\n",
      "con = 9.969367\n",
      "[epoch:84, batch:78/78] loss: 0.00037 lr: 0.01000\n",
      "lam = 0.491880\n",
      "con = 9.969896\n",
      "[epoch:85, batch:5/78] loss: 0.00046 lr: 0.01000\n",
      "lam = 0.489827\n",
      "con = 9.970576\n",
      "[epoch:85, batch:10/78] loss: 0.00062 lr: 0.01000\n",
      "lam = 0.490170\n",
      "con = 9.971026\n",
      "[epoch:85, batch:15/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.492483\n",
      "con = 9.971365\n",
      "[epoch:85, batch:20/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491331\n",
      "con = 9.971945\n",
      "[epoch:85, batch:25/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.490858\n",
      "con = 9.972493\n",
      "[epoch:85, batch:30/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.490878\n",
      "con = 9.972960\n",
      "[epoch:85, batch:35/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491159\n",
      "con = 9.973400\n",
      "[epoch:85, batch:40/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.491495\n",
      "con = 9.973825\n",
      "[epoch:85, batch:45/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.491405\n",
      "con = 9.974307\n",
      "[epoch:85, batch:50/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.491438\n",
      "con = 9.974795\n",
      "[epoch:85, batch:55/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.490301\n",
      "con = 9.975401\n",
      "[epoch:85, batch:60/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.491166\n",
      "con = 9.975880\n",
      "[epoch:85, batch:65/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491552\n",
      "con = 9.976366\n",
      "[epoch:85, batch:70/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.491196\n",
      "con = 9.976902\n",
      "[epoch:85, batch:75/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.491360\n",
      "con = 9.977181\n",
      "[epoch:85, batch:78/78] loss: 0.00034 lr: 0.01000\n",
      "lam = 0.490804\n",
      "con = 9.977674\n",
      "[epoch:86, batch:5/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.492231\n",
      "con = 9.978069\n",
      "[epoch:86, batch:10/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.490895\n",
      "con = 9.978636\n",
      "[epoch:86, batch:15/78] loss: 0.00067 lr: 0.01000\n",
      "lam = 0.490396\n",
      "con = 9.979119\n",
      "[epoch:86, batch:20/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491831\n",
      "con = 9.979475\n",
      "[epoch:86, batch:25/78] loss: 0.00057 lr: 0.01000\n",
      "lam = 0.490928\n",
      "con = 9.979980\n",
      "[epoch:86, batch:30/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.490653\n",
      "con = 9.980462\n",
      "[epoch:86, batch:35/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.491829\n",
      "con = 9.980832\n",
      "[epoch:86, batch:40/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491459\n",
      "con = 9.981267\n",
      "[epoch:86, batch:45/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.490197\n",
      "con = 9.981738\n",
      "[epoch:86, batch:50/78] loss: 0.00062 lr: 0.01000\n",
      "lam = 0.492387\n",
      "con = 9.982005\n",
      "[epoch:86, batch:55/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.492064\n",
      "con = 9.982460\n",
      "[epoch:86, batch:60/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.489789\n",
      "con = 9.983045\n",
      "[epoch:86, batch:65/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491185\n",
      "con = 9.983376\n",
      "[epoch:86, batch:70/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.492901\n",
      "con = 9.983677\n",
      "[epoch:86, batch:75/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.492921\n",
      "con = 9.983943\n",
      "[epoch:86, batch:78/78] loss: 0.00031 lr: 0.01000\n",
      "lam = 0.490759\n",
      "con = 9.984534\n",
      "[epoch:87, batch:5/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.490539\n",
      "con = 9.985001\n",
      "[epoch:87, batch:10/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.492581\n",
      "con = 9.985291\n",
      "[epoch:87, batch:15/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.491721\n",
      "con = 9.985784\n",
      "[epoch:87, batch:20/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.490087\n",
      "con = 9.986283\n",
      "[epoch:87, batch:25/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491192\n",
      "con = 9.986629\n",
      "[epoch:87, batch:30/78] loss: 0.00065 lr: 0.01000\n",
      "lam = 0.490663\n",
      "con = 9.987082\n",
      "[epoch:87, batch:35/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491100\n",
      "con = 9.987398\n",
      "[epoch:87, batch:40/78] loss: 0.00065 lr: 0.01000\n",
      "lam = 0.492354\n",
      "con = 9.987672\n",
      "[epoch:87, batch:45/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.491265\n",
      "con = 9.988128\n",
      "[epoch:87, batch:50/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.491207\n",
      "con = 9.988526\n",
      "[epoch:87, batch:55/78] loss: 0.00047 lr: 0.01000\n",
      "lam = 0.492530\n",
      "con = 9.988819\n",
      "[epoch:87, batch:60/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.491494\n",
      "con = 9.989265\n",
      "[epoch:87, batch:65/78] loss: 0.00057 lr: 0.01000\n",
      "lam = 0.492172\n",
      "con = 9.989614\n",
      "[epoch:87, batch:70/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.490355\n",
      "con = 9.990100\n",
      "[epoch:87, batch:75/78] loss: 0.00067 lr: 0.01000\n",
      "lam = 0.491077\n",
      "con = 9.990251\n",
      "[epoch:87, batch:78/78] loss: 0.00034 lr: 0.01000\n",
      "lam = 0.492361\n",
      "con = 9.990491\n",
      "[epoch:88, batch:5/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.490985\n",
      "con = 9.990961\n",
      "[epoch:88, batch:10/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.490029\n",
      "con = 9.991380\n",
      "[epoch:88, batch:15/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.492146\n",
      "con = 9.991571\n",
      "[epoch:88, batch:20/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.492119\n",
      "con = 9.991889\n",
      "[epoch:88, batch:25/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.490532\n",
      "con = 9.992283\n",
      "[epoch:88, batch:30/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.491746\n",
      "con = 9.992534\n",
      "[epoch:88, batch:35/78] loss: 0.00050 lr: 0.01000\n",
      "lam = 0.493319\n",
      "con = 9.992840\n",
      "[epoch:88, batch:40/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.491134\n",
      "con = 9.993379\n",
      "[epoch:88, batch:45/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.490051\n",
      "con = 9.993789\n",
      "[epoch:88, batch:50/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.492587\n",
      "con = 9.993974\n",
      "[epoch:88, batch:55/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.492921\n",
      "con = 9.994288\n",
      "[epoch:88, batch:60/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.490138\n",
      "con = 9.994787\n",
      "[epoch:88, batch:65/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491341\n",
      "con = 9.994996\n",
      "[epoch:88, batch:70/78] loss: 0.00066 lr: 0.01000\n",
      "lam = 0.493492\n",
      "con = 9.995152\n",
      "[epoch:88, batch:75/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.491869\n",
      "con = 9.995436\n",
      "[epoch:88, batch:78/78] loss: 0.00044 lr: 0.01000\n",
      "lam = 0.489382\n",
      "con = 9.995865\n",
      "[epoch:89, batch:5/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.493156\n",
      "con = 9.995971\n",
      "[epoch:89, batch:10/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.492140\n",
      "con = 9.996345\n",
      "[epoch:89, batch:15/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.489639\n",
      "con = 9.996803\n",
      "[epoch:89, batch:20/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.492617\n",
      "con = 9.996966\n",
      "[epoch:89, batch:25/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.492570\n",
      "con = 9.997332\n",
      "[epoch:89, batch:30/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.490317\n",
      "con = 9.997851\n",
      "[epoch:89, batch:35/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.490802\n",
      "con = 9.998168\n",
      "[epoch:89, batch:40/78] loss: 0.00066 lr: 0.01000\n",
      "lam = 0.491975\n",
      "con = 9.998382\n",
      "[epoch:89, batch:45/78] loss: 0.00052 lr: 0.01000\n",
      "lam = 0.491100\n",
      "con = 9.998735\n",
      "[epoch:89, batch:50/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491733\n",
      "con = 9.998937\n",
      "[epoch:89, batch:55/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491654\n",
      "con = 9.999124\n",
      "[epoch:89, batch:60/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.492217\n",
      "con = 9.999301\n",
      "[epoch:89, batch:65/78] loss: 0.00045 lr: 0.01000\n",
      "lam = 0.491867\n",
      "con = 9.999578\n",
      "[epoch:89, batch:70/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.492077\n",
      "con = 9.999838\n",
      "[epoch:89, batch:75/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491336\n",
      "con = 10.000025\n",
      "[epoch:89, batch:78/78] loss: 0.00036 lr: 0.01000\n",
      "lam = 0.490354\n",
      "con = 10.000315\n",
      "[epoch:90, batch:5/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.492759\n",
      "con = 10.000405\n",
      "[epoch:90, batch:10/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.490580\n",
      "con = 10.000803\n",
      "[epoch:90, batch:15/78] loss: 0.00065 lr: 0.01000\n",
      "lam = 0.491374\n",
      "con = 10.001034\n",
      "[epoch:90, batch:20/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.493639\n",
      "con = 10.001229\n",
      "[epoch:90, batch:25/78] loss: 0.00052 lr: 0.01000\n",
      "lam = 0.492240\n",
      "con = 10.001616\n",
      "[epoch:90, batch:30/78] loss: 0.00047 lr: 0.01000\n",
      "lam = 0.491835\n",
      "con = 10.001944\n",
      "[epoch:90, batch:35/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.492006\n",
      "con = 10.002217\n",
      "[epoch:90, batch:40/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.491207\n",
      "con = 10.002548\n",
      "[epoch:90, batch:45/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.491293\n",
      "con = 10.002863\n",
      "[epoch:90, batch:50/78] loss: 0.00067 lr: 0.01000\n",
      "lam = 0.492413\n",
      "con = 10.003032\n",
      "[epoch:90, batch:55/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.491986\n",
      "con = 10.003283\n",
      "[epoch:90, batch:60/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.490722\n",
      "con = 10.003565\n",
      "[epoch:90, batch:65/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.492368\n",
      "con = 10.003592\n",
      "[epoch:90, batch:70/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491042\n",
      "con = 10.003823\n",
      "[epoch:90, batch:75/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.490410\n",
      "con = 10.003961\n",
      "[epoch:90, batch:78/78] loss: 0.00037 lr: 0.01000\n",
      "lam = 0.492522\n",
      "con = 10.004007\n",
      "[epoch:91, batch:5/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.491665\n",
      "con = 10.004287\n",
      "[epoch:91, batch:10/78] loss: 0.00064 lr: 0.01000\n",
      "lam = 0.491543\n",
      "con = 10.004535\n",
      "[epoch:91, batch:15/78] loss: 0.00046 lr: 0.01000\n",
      "lam = 0.492202\n",
      "con = 10.004752\n",
      "[epoch:91, batch:20/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491081\n",
      "con = 10.005056\n",
      "[epoch:91, batch:25/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.491353\n",
      "con = 10.005228\n",
      "[epoch:91, batch:30/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.491854\n",
      "con = 10.005374\n",
      "[epoch:91, batch:35/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.490698\n",
      "con = 10.005635\n",
      "[epoch:91, batch:40/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.492592\n",
      "con = 10.005711\n",
      "[epoch:91, batch:45/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.491334\n",
      "con = 10.005975\n",
      "[epoch:91, batch:50/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.491424\n",
      "con = 10.006153\n",
      "[epoch:91, batch:55/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491294\n",
      "con = 10.006385\n",
      "[epoch:91, batch:60/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.492223\n",
      "con = 10.006521\n",
      "[epoch:91, batch:65/78] loss: 0.00049 lr: 0.01000\n",
      "lam = 0.493287\n",
      "con = 10.006651\n",
      "[epoch:91, batch:70/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.490119\n",
      "con = 10.007019\n",
      "[epoch:91, batch:75/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.490586\n",
      "con = 10.007083\n",
      "[epoch:91, batch:78/78] loss: 0.00039 lr: 0.01000\n",
      "lam = 0.493085\n",
      "con = 10.007027\n",
      "[epoch:92, batch:5/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491743\n",
      "con = 10.007219\n",
      "[epoch:92, batch:10/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491896\n",
      "con = 10.007427\n",
      "[epoch:92, batch:15/78] loss: 0.00052 lr: 0.01000\n",
      "lam = 0.491933\n",
      "con = 10.007660\n",
      "[epoch:92, batch:20/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.493704\n",
      "con = 10.007801\n",
      "[epoch:92, batch:25/78] loss: 0.00044 lr: 0.01000\n",
      "lam = 0.490217\n",
      "con = 10.008265\n",
      "[epoch:92, batch:30/78] loss: 0.00066 lr: 0.01000\n",
      "lam = 0.489978\n",
      "con = 10.008440\n",
      "[epoch:92, batch:35/78] loss: 0.00057 lr: 0.01000\n",
      "lam = 0.493805\n",
      "con = 10.008330\n",
      "[epoch:92, batch:40/78] loss: 0.00057 lr: 0.01000\n",
      "lam = 0.491546\n",
      "con = 10.008645\n",
      "[epoch:92, batch:45/78] loss: 0.00056 lr: 0.01000\n",
      "lam = 0.491521\n",
      "con = 10.008805\n",
      "[epoch:92, batch:50/78] loss: 0.00062 lr: 0.01000\n",
      "lam = 0.492118\n",
      "con = 10.008907\n",
      "[epoch:92, batch:55/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.491565\n",
      "con = 10.009096\n",
      "[epoch:92, batch:60/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.490873\n",
      "con = 10.009290\n",
      "[epoch:92, batch:65/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.492226\n",
      "con = 10.009339\n",
      "[epoch:92, batch:70/78] loss: 0.00057 lr: 0.01000\n",
      "lam = 0.491866\n",
      "con = 10.009517\n",
      "[epoch:92, batch:75/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.491622\n",
      "con = 10.009633\n",
      "[epoch:92, batch:78/78] loss: 0.00029 lr: 0.01000\n",
      "lam = 0.491903\n",
      "con = 10.009798\n",
      "[epoch:93, batch:5/78] loss: 0.00057 lr: 0.01000\n",
      "lam = 0.491716\n",
      "con = 10.009984\n",
      "[epoch:93, batch:10/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491584\n",
      "con = 10.010156\n",
      "[epoch:93, batch:15/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.492098\n",
      "con = 10.010286\n",
      "[epoch:93, batch:20/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.491303\n",
      "con = 10.010461\n",
      "[epoch:93, batch:25/78] loss: 0.00053 lr: 0.01000\n",
      "lam = 0.491432\n",
      "con = 10.010584\n",
      "[epoch:93, batch:30/78] loss: 0.00055 lr: 0.01000\n",
      "lam = 0.492698\n",
      "con = 10.010638\n",
      "[epoch:93, batch:35/78] loss: 0.00052 lr: 0.01000\n",
      "lam = 0.492593\n",
      "con = 10.010804\n",
      "[epoch:93, batch:40/78] loss: 0.00050 lr: 0.01000\n",
      "lam = 0.491091\n",
      "con = 10.011025\n",
      "[epoch:93, batch:45/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.490588\n",
      "con = 10.011156\n",
      "[epoch:93, batch:50/78] loss: 0.00067 lr: 0.01000\n",
      "lam = 0.492559\n",
      "con = 10.011162\n",
      "[epoch:93, batch:55/78] loss: 0.00063 lr: 0.01000\n",
      "lam = 0.492487\n",
      "con = 10.011287\n",
      "[epoch:93, batch:60/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.489677\n",
      "con = 10.011596\n",
      "[epoch:93, batch:65/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.494065\n",
      "con = 10.011456\n",
      "[epoch:93, batch:70/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.492182\n",
      "con = 10.011749\n",
      "[epoch:93, batch:75/78] loss: 0.00061 lr: 0.01000\n",
      "lam = 0.489651\n",
      "con = 10.012026\n",
      "[epoch:93, batch:78/78] loss: 0.00040 lr: 0.01000\n",
      "lam = 0.491859\n",
      "con = 10.012022\n",
      "[epoch:94, batch:5/78] loss: 0.00064 lr: 0.01000\n",
      "lam = 0.492512\n",
      "con = 10.012073\n",
      "[epoch:94, batch:10/78] loss: 0.00065 lr: 0.01000\n",
      "lam = 0.490188\n",
      "con = 10.012383\n",
      "[epoch:94, batch:15/78] loss: 0.00052 lr: 0.01000\n",
      "lam = 0.492791\n",
      "con = 10.012382\n",
      "[epoch:94, batch:20/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.491577\n",
      "con = 10.012615\n",
      "[epoch:94, batch:25/78] loss: 0.00048 lr: 0.01000\n",
      "lam = 0.491623\n",
      "con = 10.012737\n",
      "[epoch:94, batch:30/78] loss: 0.00060 lr: 0.01000\n",
      "lam = 0.492628\n",
      "con = 10.012778\n",
      "[epoch:94, batch:35/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491984\n",
      "con = 10.012937\n",
      "[epoch:94, batch:40/78] loss: 0.00051 lr: 0.01000\n",
      "lam = 0.491131\n",
      "con = 10.013100\n",
      "[epoch:94, batch:45/78] loss: 0.00058 lr: 0.01000\n",
      "lam = 0.492113\n",
      "con = 10.013147\n",
      "[epoch:94, batch:50/78] loss: 0.00059 lr: 0.01000\n",
      "lam = 0.492336\n",
      "con = 10.013249\n",
      "[epoch:94, batch:55/78] loss: 0.00050 lr: 0.01000\n",
      "lam = 0.490646\n",
      "con = 10.013471\n",
      "[epoch:94, batch:60/78] loss: 0.00066 lr: 0.01000\n",
      "lam = 0.492298\n",
      "con = 10.013476\n",
      "[epoch:94, batch:65/78] loss: 0.00050 lr: 0.01000\n",
      "lam = 0.492440\n",
      "con = 10.013586\n",
      "[epoch:94, batch:70/78] loss: 0.00054 lr: 0.01000\n",
      "lam = 0.491646\n",
      "con = 10.013762\n",
      "[epoch:94, batch:75/78] loss: 0.00050 lr: 0.01000\n",
      "lam = 0.492072\n",
      "con = 10.013802\n",
      "[epoch:94, batch:78/78] loss: 0.00034 lr: 0.01000\n",
      "Epoch    95: reducing learning rate of group 0 to 2.0000e-03.\n",
      "lam = 0.491811\n",
      "con = 10.013838\n",
      "[epoch:95, batch:5/78] loss: 0.00058 lr: 0.00200\n",
      "lam = 0.491413\n",
      "con = 10.013879\n",
      "[epoch:95, batch:10/78] loss: 0.00056 lr: 0.00200\n",
      "lam = 0.491280\n",
      "con = 10.013904\n",
      "[epoch:95, batch:15/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.491623\n",
      "con = 10.013890\n",
      "[epoch:95, batch:20/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.492104\n",
      "con = 10.013863\n",
      "[epoch:95, batch:25/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.492021\n",
      "con = 10.013877\n",
      "[epoch:95, batch:30/78] loss: 0.00058 lr: 0.00200\n",
      "lam = 0.491600\n",
      "con = 10.013921\n",
      "[epoch:95, batch:35/78] loss: 0.00061 lr: 0.00200\n",
      "lam = 0.491770\n",
      "con = 10.013916\n",
      "[epoch:95, batch:40/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.492076\n",
      "con = 10.013912\n",
      "[epoch:95, batch:45/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.492347\n",
      "con = 10.013923\n",
      "[epoch:95, batch:50/78] loss: 0.00049 lr: 0.00200\n",
      "lam = 0.492381\n",
      "con = 10.013964\n",
      "[epoch:95, batch:55/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.492219\n",
      "con = 10.014017\n",
      "[epoch:95, batch:60/78] loss: 0.00050 lr: 0.00200\n",
      "lam = 0.492170\n",
      "con = 10.014053\n",
      "[epoch:95, batch:65/78] loss: 0.00046 lr: 0.00200\n",
      "lam = 0.491872\n",
      "con = 10.014095\n",
      "[epoch:95, batch:70/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.491963\n",
      "con = 10.014116\n",
      "[epoch:95, batch:75/78] loss: 0.00047 lr: 0.00200\n",
      "lam = 0.491893\n",
      "con = 10.014137\n",
      "[epoch:95, batch:78/78] loss: 0.00028 lr: 0.00200\n",
      "lam = 0.492072\n",
      "con = 10.014148\n",
      "[epoch:96, batch:5/78] loss: 0.00058 lr: 0.00200\n",
      "lam = 0.492023\n",
      "con = 10.014164\n",
      "[epoch:96, batch:10/78] loss: 0.00056 lr: 0.00200\n",
      "lam = 0.491621\n",
      "con = 10.014212\n",
      "[epoch:96, batch:15/78] loss: 0.00058 lr: 0.00200\n",
      "lam = 0.491392\n",
      "con = 10.014243\n",
      "[epoch:96, batch:20/78] loss: 0.00062 lr: 0.00200\n",
      "lam = 0.491597\n",
      "con = 10.014240\n",
      "[epoch:96, batch:25/78] loss: 0.00049 lr: 0.00200\n",
      "lam = 0.492288\n",
      "con = 10.014209\n",
      "[epoch:96, batch:30/78] loss: 0.00053 lr: 0.00200\n",
      "lam = 0.492674\n",
      "con = 10.014207\n",
      "[epoch:96, batch:35/78] loss: 0.00057 lr: 0.00200\n",
      "lam = 0.492238\n",
      "con = 10.014254\n",
      "[epoch:96, batch:40/78] loss: 0.00066 lr: 0.00200\n",
      "lam = 0.491623\n",
      "con = 10.014330\n",
      "[epoch:96, batch:45/78] loss: 0.00050 lr: 0.00200\n",
      "lam = 0.491436\n",
      "con = 10.014370\n",
      "[epoch:96, batch:50/78] loss: 0.00050 lr: 0.00200\n",
      "lam = 0.491629\n",
      "con = 10.014386\n",
      "[epoch:96, batch:55/78] loss: 0.00056 lr: 0.00200\n",
      "lam = 0.491836\n",
      "con = 10.014397\n",
      "[epoch:96, batch:60/78] loss: 0.00052 lr: 0.00200\n",
      "lam = 0.491933\n",
      "con = 10.014408\n",
      "[epoch:96, batch:65/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.491832\n",
      "con = 10.014432\n",
      "[epoch:96, batch:70/78] loss: 0.00047 lr: 0.00200\n",
      "lam = 0.491586\n",
      "con = 10.014466\n",
      "[epoch:96, batch:75/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.491723\n",
      "con = 10.014473\n",
      "[epoch:96, batch:78/78] loss: 0.00029 lr: 0.00200\n",
      "lam = 0.492285\n",
      "con = 10.014452\n",
      "[epoch:97, batch:5/78] loss: 0.00054 lr: 0.00200\n",
      "lam = 0.492292\n",
      "con = 10.014459\n",
      "[epoch:97, batch:10/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.492372\n",
      "con = 10.014472\n",
      "[epoch:97, batch:15/78] loss: 0.00048 lr: 0.00200\n",
      "lam = 0.492203\n",
      "con = 10.014511\n",
      "[epoch:97, batch:20/78] loss: 0.00047 lr: 0.00200\n",
      "lam = 0.491819\n",
      "con = 10.014567\n",
      "[epoch:97, batch:25/78] loss: 0.00057 lr: 0.00200\n",
      "lam = 0.491972\n",
      "con = 10.014568\n",
      "[epoch:97, batch:30/78] loss: 0.00052 lr: 0.00200\n",
      "lam = 0.492314\n",
      "con = 10.014550\n",
      "[epoch:97, batch:35/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.492005\n",
      "con = 10.014587\n",
      "[epoch:97, batch:40/78] loss: 0.00048 lr: 0.00200\n",
      "lam = 0.491693\n",
      "con = 10.014625\n",
      "[epoch:97, batch:45/78] loss: 0.00057 lr: 0.00200\n",
      "lam = 0.491514\n",
      "con = 10.014657\n",
      "[epoch:97, batch:50/78] loss: 0.00052 lr: 0.00200\n",
      "lam = 0.491778\n",
      "con = 10.014659\n",
      "[epoch:97, batch:55/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.491524\n",
      "con = 10.014698\n",
      "[epoch:97, batch:60/78] loss: 0.00064 lr: 0.00200\n",
      "lam = 0.491362\n",
      "con = 10.014741\n",
      "[epoch:97, batch:65/78] loss: 0.00052 lr: 0.00200\n",
      "lam = 0.491421\n",
      "con = 10.014770\n",
      "[epoch:97, batch:70/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.491791\n",
      "con = 10.014776\n",
      "[epoch:97, batch:75/78] loss: 0.00051 lr: 0.00200\n",
      "lam = 0.492030\n",
      "con = 10.014774\n",
      "[epoch:97, batch:78/78] loss: 0.00035 lr: 0.00200\n",
      "lam = 0.492465\n",
      "con = 10.014763\n",
      "[epoch:98, batch:5/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.492193\n",
      "con = 10.014802\n",
      "[epoch:98, batch:10/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.491702\n",
      "con = 10.014871\n",
      "[epoch:98, batch:15/78] loss: 0.00051 lr: 0.00200\n",
      "lam = 0.491777\n",
      "con = 10.014893\n",
      "[epoch:98, batch:20/78] loss: 0.00052 lr: 0.00200\n",
      "lam = 0.492164\n",
      "con = 10.014885\n",
      "[epoch:98, batch:25/78] loss: 0.00048 lr: 0.00200\n",
      "lam = 0.492065\n",
      "con = 10.014904\n",
      "[epoch:98, batch:30/78] loss: 0.00054 lr: 0.00200\n",
      "lam = 0.491560\n",
      "con = 10.014955\n",
      "[epoch:98, batch:35/78] loss: 0.00063 lr: 0.00200\n",
      "lam = 0.491591\n",
      "con = 10.014957\n",
      "[epoch:98, batch:40/78] loss: 0.00057 lr: 0.00200\n",
      "lam = 0.491506\n",
      "con = 10.014968\n",
      "[epoch:98, batch:45/78] loss: 0.00060 lr: 0.00200\n",
      "lam = 0.491939\n",
      "con = 10.014959\n",
      "[epoch:98, batch:50/78] loss: 0.00056 lr: 0.00200\n",
      "lam = 0.492163\n",
      "con = 10.014969\n",
      "[epoch:98, batch:55/78] loss: 0.00053 lr: 0.00200\n",
      "lam = 0.492098\n",
      "con = 10.015001\n",
      "[epoch:98, batch:60/78] loss: 0.00049 lr: 0.00200\n",
      "lam = 0.491686\n",
      "con = 10.015051\n",
      "[epoch:98, batch:65/78] loss: 0.00061 lr: 0.00200\n",
      "lam = 0.491187\n",
      "con = 10.015102\n",
      "[epoch:98, batch:70/78] loss: 0.00057 lr: 0.00200\n",
      "lam = 0.491626\n",
      "con = 10.015085\n",
      "[epoch:98, batch:75/78] loss: 0.00053 lr: 0.00200\n",
      "lam = 0.492251\n",
      "con = 10.015059\n",
      "[epoch:98, batch:78/78] loss: 0.00030 lr: 0.00200\n",
      "lam = 0.492652\n",
      "con = 10.015062\n",
      "[epoch:99, batch:5/78] loss: 0.00060 lr: 0.00200\n",
      "lam = 0.492116\n",
      "con = 10.015117\n",
      "[epoch:99, batch:10/78] loss: 0.00054 lr: 0.00200\n",
      "lam = 0.491687\n",
      "con = 10.015159\n",
      "[epoch:99, batch:15/78] loss: 0.00054 lr: 0.00200\n",
      "lam = 0.491516\n",
      "con = 10.015191\n",
      "[epoch:99, batch:20/78] loss: 0.00050 lr: 0.00200\n",
      "lam = 0.491115\n",
      "con = 10.015232\n",
      "[epoch:99, batch:25/78] loss: 0.00071 lr: 0.00200\n",
      "lam = 0.490988\n",
      "con = 10.015246\n",
      "[epoch:99, batch:30/78] loss: 0.00056 lr: 0.00200\n",
      "lam = 0.491675\n",
      "con = 10.015205\n",
      "[epoch:99, batch:35/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.492325\n",
      "con = 10.015173\n",
      "[epoch:99, batch:40/78] loss: 0.00056 lr: 0.00200\n",
      "lam = 0.492082\n",
      "con = 10.015212\n",
      "[epoch:99, batch:45/78] loss: 0.00055 lr: 0.00200\n",
      "lam = 0.491994\n",
      "con = 10.015255\n",
      "[epoch:99, batch:50/78] loss: 0.00052 lr: 0.00200\n",
      "lam = 0.492031\n",
      "con = 10.015281\n",
      "[epoch:99, batch:55/78] loss: 0.00050 lr: 0.00200\n",
      "lam = 0.492290\n",
      "con = 10.015295\n",
      "[epoch:99, batch:60/78] loss: 0.00049 lr: 0.00200\n",
      "lam = 0.492447\n",
      "con = 10.015312\n",
      "[epoch:99, batch:65/78] loss: 0.00059 lr: 0.00200\n",
      "lam = 0.492402\n",
      "con = 10.015334\n",
      "[epoch:99, batch:70/78] loss: 0.00050 lr: 0.00200\n",
      "lam = 0.492335\n",
      "con = 10.015361\n",
      "[epoch:99, batch:75/78] loss: 0.00053 lr: 0.00200\n",
      "lam = 0.492029\n",
      "con = 10.015393\n",
      "[epoch:99, batch:78/78] loss: 0.00035 lr: 0.00200\n",
      "Epoch   100: reducing learning rate of group 0 to 4.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "fit(criterionFunc=\"LogMSE\", epochMax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}